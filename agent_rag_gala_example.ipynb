{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Best Gala Agent\n",
    "\n",
    "- We decided to host the most extravagant and opulent party of the century. This means lavish feasts, enchanting dancers, renowned DJs, exquisite drinks, a breathtaking fireworks display, and much more.\n",
    "- Our party planner agent will manage everything by himself. To do so, he needs to have access to all of the information about the party, including the menu, the guests, the schedule, weather forecasts, and much more!\n",
    "- He also needs to be able to answer any questions about the party during the party\n",
    "- Requirements:\n",
    "    - Be aware of topics, like politics and religion, that are to be avoided at a gala\n",
    "    - A good host should be aware of guestsâ€™ backgrounds\n",
    "    - Some general knowledge about the weather to ensure we can continuously find a real-time update to ensure perfect timing to launch the fireworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why RAG for a Gala?\n",
    "\n",
    "During the party, our agent will need to recall specific details about each person at a momentâ€™s notice. A traditional LLM might struggle with this task because:\n",
    "\n",
    "1. The guest list is specific to your event and not in the modelâ€™s training data\n",
    "2. Guest information may change or be updated frequently\n",
    "3. Alfred needs to retrieve precise details like email addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load & Prepare the Data\n",
    "\n",
    "import datasets\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load the dataset\n",
    "guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")\n",
    "\n",
    "# Convert dataset entries into Document objects\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"\\n\".join([\n",
    "            f\"Name: {guest['name']}\",\n",
    "            f\"Relation: {guest['relation']}\",\n",
    "            f\"Description: {guest['description']}\",\n",
    "            f\"Email: {guest['email']}\"\n",
    "        ]),\n",
    "        metadata={\"name\": guest[\"name\"]}\n",
    "    )\n",
    "    for guest in guest_dataset\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'name': 'Ada Lovelace'}, page_content=\"Name: Ada Lovelace\\nRelation: best friend\\nDescription: Lady Ada Lovelace is my best friend. She is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine.\\nEmail: ada.lovelace@example.com\"),\n",
       " Document(metadata={'name': 'Dr. Nikola Tesla'}, page_content=\"Name: Dr. Nikola Tesla\\nRelation: old friend from university days\\nDescription: Dr. Nikola Tesla is an old friend from your university days. He's recently patented a new wireless energy transmission system and would be delighted to discuss it with you. Just remember he's passionate about pigeons, so that might make for good small talk.\\nEmail: nikola.tesla@gmail.com\"),\n",
       " Document(metadata={'name': 'Marie Curie'}, page_content='Name: Marie Curie\\nRelation: no relation\\nDescription: Marie Curie was a groundbreaking physicist and chemist, famous for her research on radioactivity.\\nEmail: marie.curie@example.com')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a Retriever Tool\n",
    "# We will use the BM25Retriever from the langchain_community.retrievers module to create a retriever tool.\n",
    "\n",
    "\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.tools import Tool\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "\n",
    "def extract_text(query: str) -> str:\n",
    "    \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\"\n",
    "    results = bm25_retriever.invoke(query)\n",
    "    if results:\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in results[:3]])\n",
    "    else:\n",
    "        return \"No matching guest information found.\"\n",
    "\n",
    "guest_info_tool = Tool(\n",
    "    name=\"guest_info_retriever\",\n",
    "    func=extract_text,\n",
    "    description=\"Retrieves detailed information about gala guests based on their name or relation.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This tool: \n",
    "- The name & description help the agent understand when and how to use this tool\n",
    "- The type decorator define what parameters the tool expects (in this case a search query)\n",
    "- BM25 doesn't require embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Generate the chat interface, including the tools\n",
    "llm = \"qwen2.5-coder:7b\"\n",
    "\n",
    "chat = ChatOllama(model=llm, verbose=True)\n",
    "tools = [guest_info_tool]\n",
    "chat_with_tools = chat.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ© Party Planner's Response:\n",
      "Ada Lovelace was an esteemed mathematician and friend who is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine. Her email address is ada.lovelace@example.com.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Integrate the Tool with the Agent\n",
    "\n",
    "# Generate the AgentState and Agent graph\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def assistant(state: AgentState):\n",
    "    return {\"\"\n",
    "        \"messages\": [chat_with_tools.invoke(state[\"messages\"])],\n",
    "    }\n",
    "\n",
    "## The graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message requires a tool, route to tools\n",
    "    # Otherwise, provide a direct response\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "agent = builder.compile()\n",
    "\n",
    "messages = [HumanMessage(content=\"Tell me about our guest named 'Ada Lovelace'. Make sure to not include information about other guests.\")]\n",
    "response = agent.invoke({\"messages\": messages})\n",
    "\n",
    "print(\"ðŸŽ© Party Planner's Response:\")\n",
    "print(response['messages'][-1].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Integrating Tools for Agent\n",
    "\n",
    "- Web Search Tool\n",
    "- Creating a Custom Tool for Weather Information to Schedule the Fireworks\n",
    "- Creating a Hub Stats Tool for Influential AI Builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emmanuel Macron (born December 21, 1977, Amiens, France) is a French banker and politician who was elected president of France in 2017. Macron was the first person in the history of the Fifth Republic to win the presidency without the backing of either the Socialists or the Gaullists, and he was France's youngest head of state since Napoleon I.He was reelected in 2022, becoming the first ... The current President of France is Emmanuel Macron, who has held office since being elected in the 2017 French Presidential Election. The Prime Minister of France is the leader of government and holds the power to manage the numerous public agencies based around the nation. The French Executive Cabinet is formally comprised of members of ... PARIS (AP) â€” French President Emmanuel Macron vowed Thursday to stay in office until the end of his term, due in 2027, and announced that he will name a new prime minister within days following ... Emmanuel Macron is the current President of France, serving since 2017. He was born on December 21, 1977, in Amiens, France. Macron is married to Brigitte Macron, and together they have become a prominent couple in French politics and culture. He is known for his centrist political stance and his efforts to reform the French economy and ... French President Emmanuel Macron has unveiled his new government almost three months after a snap general election delivered a hung parliament. The long-awaited new line up, led by Prime Minister ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "results = search_tool.invoke(\"Who's the current President of France?\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "import random\n",
    "\n",
    "def get_weather_info(location: str) -> str:\n",
    "    \"\"\"Fetches dummy weather information for a given location.\"\"\"\n",
    "    # Dummy weather data\n",
    "    weather_conditions = [\n",
    "        {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
    "        {\"condition\": \"Clear\", \"temp_c\": 25},\n",
    "        {\"condition\": \"Windy\", \"temp_c\": 20}\n",
    "    ]\n",
    "    # Randomly select a weather condition\n",
    "    data = random.choice(weather_conditions)\n",
    "    return f\"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C\"\n",
    "\n",
    "# Initialize the tool\n",
    "weather_info_tool = Tool(\n",
    "    name=\"get_weather_info\",\n",
    "    func=get_weather_info,\n",
    "    description=\"Fetches dummy weather information for a given location.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since there are influential AI builders, our party planner agent should impress them by discussing their most popular models and pieces of works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most downloaded model by facebook is facebook/esmfold_v1 with 13,602,902 downloads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericw\\AppData\\Local\\Temp\\ipykernel_25428\\4027241918.py:25: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(hub_stats_tool(\"facebook\")) # Example: Get the most downloaded model by Facebook\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import list_models\n",
    "\n",
    "def get_hub_stats(author: str) -> str:\n",
    "    \"\"\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\"\"\n",
    "    try:\n",
    "        # List models from the specified author, sorted by downloads\n",
    "        models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1))\n",
    "\n",
    "        if models:\n",
    "            model = models[0]\n",
    "            return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\"\n",
    "        else:\n",
    "            return f\"No models found for author {author}.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching models for {author}: {str(e)}\"\n",
    "\n",
    "# Initialize the tool\n",
    "hub_stats_tool = Tool(\n",
    "    name=\"get_hub_stats\",\n",
    "    func=get_hub_stats,\n",
    "    description=\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "print(hub_stats_tool(\"facebook\")) # Example: Get the most downloaded model by Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's integrate these tools into our party planner agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ© Party Planner's Response:\n",
      "Based on the provided information, here's a summary:\n",
      "\n",
      "Google is a multinational technology company known for its search engine, which has been a cornerstone of its business model. The company also develops and releases AI models that are designed to enhance various functionalities.\n",
      "\n",
      "### Most Popular Model:\n",
      "Google has recently announced an \"enhanced\" reasoning mode for its flagship Gemini 2.5 Pro model called Deep Think. Additionally, the company released Gemini 2.0, which is described as their most capable artificial intelligence model suite yet.\n",
      "\n",
      "These advancements suggest that Google continues to push the boundaries of AI capabilities and aims to provide more efficient and accurate services to its users through its innovative models.\n",
      "\n",
      "### Key Points:\n",
      "1. **Initial Business Model**: Google initially built a powerful search engine using advanced algorithms.\n",
      "2. **AI Models**: The company is known for developing and releasing AI models like Gemini 2.5 Pro (with Deep Think) and Gemini 2.0.\n",
      "3. **Data Analytics**: Central to Google's business model is its reliance on data analytics and artificial intelligence, which enables personalized services and improved advertising.\n",
      "\n",
      "This summary should provide a good overview of Google and their focus on AI models.\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "# Generate the chat interface, including the tools\n",
    "llm = \"qwen2.5-coder:7b\"\n",
    "\n",
    "chat = ChatOllama(model=llm, verbose=True)\n",
    "\n",
    "tools = [search_tool, weather_info_tool, hub_stats_tool]\n",
    "chat_with_tools = chat.bind_tools(tools)\n",
    "\n",
    "# Generate the AgentState and Agent graph\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def assistant(state: AgentState):\n",
    "    return {\n",
    "        \"messages\": [chat_with_tools.invoke(state[\"messages\"])],\n",
    "    }\n",
    "\n",
    "## The graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message requires a tool, route to tools\n",
    "    # Otherwise, provide a direct response\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "alfred = builder.compile()\n",
    "\n",
    "messages = [HumanMessage(content=\"Who is Google and what's their most popular model?\")]\n",
    "response = alfred.invoke({\"messages\": messages})\n",
    "\n",
    "print(\"ðŸŽ© Party Planner's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-agent-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
